{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Fr2zyRoom/ISLES2017_LesionSegmentation_Tutorial/blob/main/handling_ISLES2017_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISLES2017 Lesion Segmentation Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=1ItQhiEEKMC-xXwhcch-0qPJ7iw00IA0g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL.Image as Image\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About ISLES 2017 data\n",
    "Lesion segmentation task: MRI DWI, ADC를 input data로 하여 lesion segmentation을 output으로 내는 challenge  \n",
    "현대의 MR technique(diffusion/perfusion imaging)이 acutely infarcted tissue(“core”)와 hypo-perfused lesion tissue(“penumbra”)를 잘 구별할 수 있지만, 지금의 자동화 모델은 한계가 존재\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening\n",
    "- Step 1. ISLES 2017 data specifications\n",
    "- Step 2. nii 파일 불러오기: nibabel\n",
    "    - Step 2-1. NIFTI header\n",
    "    - Step 2-2. NIFTI image\n",
    "    - Step 2-3. Ground truth\n",
    "- Step 3. Pre-processing & Visualization\n",
    "    - Step 3-1. Normalization\n",
    "    - Step 3-2. Data cleansing\n",
    "- Step 4. Save images to .npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. ISLES 2017 data specifications\n",
    "데이터 구성은 DWI, ADC, perfusion map, OT - ground truth(lesion mask)로,  \n",
    "각각의 디렉토리(폴더)에 nii 확장자로 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ./data/ISLES2017_Training/training_1/ -L 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/ISLES2017_Training.csv\")\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. nii 파일 불러오기: nibabel\n",
    "NIFTI 파일은 영상과 영상의 info가 저장된 header로 구성되어 있습니다.  \n",
    "Python에서 NIFTI 파일은 nifti package를 통해 열어볼 수 있습니다.  \n",
    "먼저 DWI 부터 열어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 DWI 데이터를 open\n",
    "sample_path = './data/ISLES2017_Training/training_1/VSD.Brain.XX.O.MR_4DPWI.127015/VSD.Brain.XX.O.MR_4DPWI.127015.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call nii image (proxy)\n",
    "ni_img = nib.load(sample_path)\n",
    "ni_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-1. NIFTI header\n",
    "nibabel.load를 통해 불러온 Nifti1Image에 .header를 붙여 header를 불러올 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call nii header\n",
    "ni_header = ni_img.header\n",
    "print(ni_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get method가 존재하여 원하는 header 정보를 불러올 수 있음.\n",
    "ni_header.get('magic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-2. NIFTI image\n",
    "nibabel.load를 통해 불러온 Nifti1Image에 .get_fdata()를 붙여 numpy 형태의 영상을 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_img_data = ni_img.get_fdata()\n",
    "ni_img_data.shape #(Width, Height, Slices, 1, Acquisition Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file is four dimensional. \n",
    "#The fourth dimension corresponds to the different diffusion orientation probed during the scan.\n",
    "ni_img_data = np.squeeze(ni_img_data) # 1 제거\n",
    "ni_img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제의 편의를 위해 전치\n",
    "ni_img_data_tp = ni_img_data.transpose()\n",
    "ni_img_data_tp.shape #(Acquisition Number, Slices, Height(row), Width(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_img_data_sq20 = ni_img_data_tp[20]\n",
    "ni_img_data_sq20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_img_data_sq20_slice11 = ni_img_data_sq20[10]\n",
    "ni_img_data_sq20_slice11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ni_img_data_sq20_slice11, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-3. Ground truth\n",
    "ISLES 2017 dataset은 lesion mask 또한 NIFTI로 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = './data/ISLES2017_Training/training_1/VSD.Brain.XX.O.OT.128050/VSD.Brain.XX.O.OT.128050.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_img = nib.load(gt_path)\n",
    "gt_img_data = gt_img.get_fdata()\n",
    "gt_img_data.shape #(Width, Height, Slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_img_data_tp = gt_img_data.transpose()\n",
    "gt_img_data_tp.shape #(Slices, Height(row), Width(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(10,10))\n",
    "\n",
    "ax[0].set_title('slice 11')\n",
    "ax[0].imshow(ni_img_data_sq20_slice11, cmap='gray')\n",
    "\n",
    "ax[1].set_title('slice 11 with mask')\n",
    "ax[1].imshow(ni_img_data_sq20_slice11, cmap='gray')\n",
    "ax[1].imshow(gt_img_data_tp[10], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Pre-processing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_sample_stack(stack, rows=3, cols=6, start_with=0, show_every=1):\n",
    "    fig,ax = plt.subplots(rows,cols,figsize=[18,20])\n",
    "    for i in range(rows*cols):\n",
    "        ind = start_with + i*show_every\n",
    "        ax[int(i/cols),int(i % cols)].set_title(f'slice {ind}')\n",
    "        ax[int(i/cols),int(i % cols)].imshow(stack[ind], cmap='gray')\n",
    "        ax[int(i/cols),int(i % cols)].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sample_stack(ni_img_data_sq20) #이 경우 각 slice의 밝기 차이가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제의 편의를 위해 전치\n",
    "ni_img_data_tp = ni_img_data.transpose()\n",
    "ni_img_data_tp.shape #(Acquisition Number, Slices, Height(row), Width(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_img_data_sq20 = ni_img_data_tp[20]\n",
    "ni_img_data_sq20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 케이스\n",
    "sample_path2 = './data/ISLES2017_Training/training_36/SMIR.Brain.XX.O.MR_4DPWI.188863/SMIR.Brain.XX.O.MR_4DPWI.188863.nii'\n",
    "ni_img2 = nib.load(sample_path2)\n",
    "ni_img_data2 = ni_img2.get_fdata()\n",
    "ni_img_data2 = np.squeeze(ni_img_data2)\n",
    "ni_img_data2 = ni_img_data2.transpose()\n",
    "ni_img_data2_sq0 = ni_img_data2[0]\n",
    "w_sample_stack(ni_img_data2_sq0) #이 경우 각 slice의 밝기 차이가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stack(stack, rows=3, cols=6, start_with=0, show_every=1, vmin=0, vmax=255):\n",
    "    fig,ax = plt.subplots(rows,cols,figsize=[18,20])\n",
    "    for i in range(rows*cols):\n",
    "        ind = start_with + i*show_every\n",
    "        ax[int(i/cols),int(i % cols)].set_title(f'slice {ind}')\n",
    "        ax[int(i/cols),int(i % cols)].imshow(stack[ind], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        ax[int(i/cols),int(i % cols)].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ni_img_data2_sq0.min(), ni_img_data2_sq0.max())\n",
    "sample_stack(ni_img_data2_sq0, vmin=ni_img_data2_sq0.min(), vmax=ni_img_data2_sq0.max()) \n",
    "#3D image에서의 min max값으로 고정을 해야 visualize가 잘됨. plt 특성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ni_img_data2_sq0[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "print(ni_img_data2_sq0[0].min(), ni_img_data2_sq0[0].max())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(ni_img_data2_sq0[0].flatten())\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow는 영상의 최소 최대값으로 normalize하여 plot하기 때문에 intensity는 6이지만 밝게 보이는 것\n",
    "# 따라서 plt.imshow의 parameter로 vmin, vmax값을 지정해줘야합니다.\n",
    "# 주의: 실제 값은 상대적으로 작지만 plt.imshow로 볼 때만 밝게 보이는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-1. Normalization\n",
    "모든 데이터 포인트가 동일한 정도의 스케일(중요도)로 반영되도록 해주는 게 정규화(Normalization)의 목표\n",
    "\n",
    "Normalize 방법은 여럿 있지만 대표적으로 \n",
    "1. Min-Max Normalization (최소-최대 정규화)  \n",
    "(X - MIN) / (MAX-MIN) \n",
    "2. Z-Score Normalization (Z-점수 정규화)  \n",
    "\n",
    "가 있습니다. 본 tutorial에서는 1번을 활용 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR과 같은 3D image를 min-max normalize할 때는 각 slice마다 normalize하는 것이 아니라  \n",
    "# 전체 slices에 대해 min-max normalize를 진행해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img_arr):\n",
    "    norm_arr = (img_arr-img_arr.min())/(img_arr.max()-img_arr.min())*255\n",
    "    return norm_arr.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_img_data2_sq0_nrm = normalize(ni_img_data2_sq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(ni_img_data2_sq0[0].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(ni_img_data2_sq0_nrm[0].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stack(ni_img_data2_sq0_nrm, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-2. Data cleansing\n",
    "주어진 table 데이터와 영상 데이터의 매치가 잘 되는지 누락된 데이터는 없는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/ISLES2017_Training.csv\")\n",
    "train_data_dir = './data/ISLES2017_Training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolders_of(directory):\n",
    "    \"\"\"find subfolders in dir.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str) -- folder directory\n",
    "    \n",
    "    Return:\n",
    "        subfolder_ls (list) -- list of 'subfolders' in dir\n",
    "    \"\"\"\n",
    "    subfolder_ls = []\n",
    "    with os.scandir(directory) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_dir():\n",
    "                subfolder_ls.append(os.path.join(directory,entry.name))\n",
    "    return subfolder_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfolder 확인\n",
    "train_subfolder_ls = [os.path.basename(p) for p in get_subfolders_of(train_data_dir)]\n",
    "train_subfolder_ls = sorted(train_subfolder_ls)\n",
    "len(train_subfolder_ls), train_subfolder_ls[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe과 영상 데이터가 실제로 매치되는지 확인\n",
    "train_df[\"DataExist\"] = train_df[\"Case SMIR ID 1\"].map(lambda x: 1 if x in train_subfolder_ls else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.DataExist.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 데이터 없는 행 제거\n",
    "train_df_clr = train_df[train_df.DataExist == 1].reset_index(drop=True)\n",
    "len(train_df_clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clr.to_csv(\"./data/ISLES2017_Training_clr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"./data/ISLES2017_Training_clr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRS Score 분포\n",
    "train_df_clr.MRSScore.value_counts().sort_values().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Save images to .npz\n",
    "모델 학습에 용이하도록 2d slice를 npz로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.dcm', '.DCM', '.raw', '.RAW', '.svs', '.SVS']\n",
    "IMG_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "DCM_EXTENSION = ['.dcm', '.DCM']\n",
    "RAW_EXTENSION = ['.raw', '.RAW']\n",
    "NIFTI_EXTENSION = ['.nii']\n",
    "\n",
    "\n",
    "def check_extension(filename, extension_ls=FILE_EXTENSION):\n",
    "    return any(filename.endswith(extension) for extension in extension_ls)\n",
    "\n",
    "def load_file_path(folder_path, extension_ls=FILE_EXTENSION, find_all=False):\n",
    "    \"\"\"find 'IMG_EXTENSION' file paths in folder.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str) -- folder directory\n",
    "        extension_ls (list) -- list of extensions\n",
    "    \n",
    "    Return:\n",
    "        file_paths (list) -- list of 'extension_ls' file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    assert os.path.isdir(folder_path), f'{folder_path} is not a valid directory'\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(folder_path)):\n",
    "        for fname in fnames:\n",
    "            if check_extension(fname, extension_ls):\n",
    "                path = os.path.join(root, fname)\n",
    "                file_paths.append(path)\n",
    "        if not find_all:\n",
    "            break\n",
    "\n",
    "    return file_paths[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_dir(new_dir):\n",
    "    try: \n",
    "        if not os.path.exists(new_dir): \n",
    "            os.makedirs(new_dir) \n",
    "            #print(f\"New directory!: {new_dir}\")\n",
    "    except OSError: \n",
    "        print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_modality_in(folder_name, mod='4DPWI'):\n",
    "    if folder_name.split('.')[-2].split('_')[-1] == mod:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_subfolders_of(directory):\n",
    "    subfolder_ls = []\n",
    "    with os.scandir(directory) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_dir():\n",
    "                subfolder_ls.append(os.path.join(directory,entry.name))\n",
    "    return subfolder_ls\n",
    "\n",
    "def find_modality_dir(data_dir, mod='4DPWI'):\n",
    "    modality_ls = get_subfolders_of(data_dir)\n",
    "    for modality in modality_ls:\n",
    "        if check_modality_in(os.path.basename(modality), mod):\n",
    "            return modality\n",
    "    return None\n",
    "\n",
    "def get_isles_data(dataset_dir, mod='4DPWI'):\n",
    "    data_path_ls = []\n",
    "    case_ls = get_subfolders_of(dataset_dir)\n",
    "    for case_dir in case_ls:\n",
    "        modality = find_modality_dir(case_dir, mod)\n",
    "        if modality:\n",
    "            data_path_ls.append(load_file_path(modality, NIFTI_EXTENSION)[0])\n",
    "    return data_path_ls\n",
    "\n",
    "def get_isles_data_with_gt(dataset_dir, mod='4DPWI'):\n",
    "    data_path_ls = []\n",
    "    case_ls = get_subfolders_of(dataset_dir)\n",
    "    for case_dir in case_ls:\n",
    "        modality = find_modality_dir(case_dir, mod)\n",
    "        gt = find_modality_dir(case_dir, 'OT')\n",
    "        if modality:\n",
    "            data_path_ls.append([load_file_path(modality, NIFTI_EXTENSION)[0], \n",
    "                                 load_file_path(gt, NIFTI_EXTENSION)[0]])\n",
    "    return data_path_ls\n",
    "\n",
    "def normalize(img_arr):\n",
    "    norm_arr = (img_arr-img_arr.min())/(img_arr.max()-img_arr.min())*255\n",
    "    return norm_arr.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_isles_dwi_dataset_to_np(save_dir, dataset_dir):\n",
    "    gen_new_dir(save_dir)\n",
    "    isles_dataset = get_isles_data_with_gt(dataset_dir, mod='4DPWI')\n",
    "    for data, gt in isles_dataset:\n",
    "        fname = data.split('/')[-3]\n",
    "        save_fname = os.path.join(save_dir, fname)\n",
    "        gen_new_dir(save_fname)\n",
    "        dwi_img_sq = np.squeeze(nib.load(data).get_fdata())\n",
    "        gt_img = nib.load(gt).get_fdata()\n",
    "        dwi_img_sq = dwi_img_sq.transpose(3,2,1,0)\n",
    "        gt_img = gt_img.transpose(2,1,0).astype(np.uint8)\n",
    "        for sq_idx, dwi_img in enumerate(dwi_img_sq):\n",
    "            dwi_save_point = os.path.join(save_fname,'dwi',str(sq_idx).zfill(3))\n",
    "            gt_save_point = os.path.join(save_fname,'mask',str(sq_idx).zfill(3))\n",
    "            gen_new_dir(dwi_save_point)\n",
    "            gen_new_dir(gt_save_point)\n",
    "            for slice_idx in range(len(dwi_img)):\n",
    "                np.save(os.path.join(dwi_save_point, str(slice_idx).zfill(3)+'.npy'), dwi_img[slice_idx])\n",
    "                np.save(os.path.join(gt_save_point, str(slice_idx).zfill(3)+'.npy'), gt_img[slice_idx])\n",
    "        print(f'Saved! {fname}(sq:{len(dwi_img_sq)} / slices:{len(dwi_img)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_isles_dwi_dataset_to_png(save_dir, dataset_dir):\n",
    "    gen_new_dir(save_dir)\n",
    "    isles_dataset = get_isles_data_with_gt(dataset_dir, mod='4DPWI')\n",
    "    for data, gt in isles_dataset:\n",
    "        fname = data.split('/')[-3]\n",
    "        save_fname = os.path.join(save_dir, fname)\n",
    "        gen_new_dir(save_fname)\n",
    "        dwi_img_sq = np.squeeze(nib.load(data).get_fdata())\n",
    "        gt_img = nib.load(gt).get_fdata()\n",
    "        dwi_img_sq = dwi_img_sq.transpose(3,2,1,0)\n",
    "        gt_img = normalize(gt_img.transpose(2,1,0))\n",
    "        for sq_idx, dwi_img in enumerate(dwi_img_sq):\n",
    "            dwi_save_point = os.path.join(save_fname,'dwi',str(sq_idx).zfill(3))\n",
    "            gt_save_point = os.path.join(save_fname,'mask',str(sq_idx).zfill(3))\n",
    "            gen_new_dir(dwi_save_point)\n",
    "            gen_new_dir(gt_save_point)\n",
    "            dwi_img = normalize(dwi_img)\n",
    "            for slice_idx in range(len(dwi_img)):\n",
    "                Image.fromarray(dwi_img[slice_idx]).save(os.path.join(dwi_save_point, str(slice_idx).zfill(3)+'.png'))\n",
    "                Image.fromarray(gt_img[slice_idx]).save(os.path.join(gt_save_point, str(slice_idx).zfill(3)+'.png'))\n",
    "        print(f'Saved! {fname}(sq:{len(dwi_img_sq)} / slices:{len(dwi_img)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_isles_dwi_dataset_to_np_norm(save_dir, dataset_dir):\n",
    "    gen_new_dir(save_dir)\n",
    "    isles_dataset = get_isles_data_with_gt(dataset_dir, mod='4DPWI')\n",
    "    for data, gt in isles_dataset:\n",
    "        fname = data.split('/')[-3]\n",
    "        save_fname = os.path.join(save_dir, fname)\n",
    "        gen_new_dir(save_fname)\n",
    "        dwi_img_sq = np.squeeze(nib.load(data).get_fdata())\n",
    "        gt_img = nib.load(gt).get_fdata()\n",
    "        dwi_img_sq = dwi_img_sq.transpose(3,2,1,0)\n",
    "        gt_img = gt_img.transpose(2,1,0).astype(np.uint8)\n",
    "        for sq_idx, dwi_img in enumerate(dwi_img_sq):\n",
    "            dwi_save_point = os.path.join(save_fname,'dwi',str(sq_idx).zfill(3))\n",
    "            gt_save_point = os.path.join(save_fname,'mask',str(sq_idx).zfill(3))\n",
    "            gen_new_dir(dwi_save_point)\n",
    "            gen_new_dir(gt_save_point)\n",
    "            dwi_img_norm = normalize(dwi_img)\n",
    "            for slice_idx in range(len(dwi_img_norm)):\n",
    "                np.save(os.path.join(dwi_save_point, str(slice_idx).zfill(3)+'.npy'), dwi_img_norm[slice_idx])\n",
    "                np.save(os.path.join(gt_save_point, str(slice_idx).zfill(3)+'.npy'), gt_img[slice_idx])\n",
    "        print(f'Saved! {fname}(sq:{len(dwi_img_sq)} / slices:{len(dwi_img_norm)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_isles_adc_dataset_to_np(save_dir, dataset_dir):\n",
    "    gen_new_dir(save_dir)\n",
    "    isles_dataset = get_isles_data_with_gt(dataset_dir, mod='ADC')\n",
    "    for data, gt in isles_dataset:\n",
    "        fname = data.split('/')[-3]\n",
    "        save_fname = os.path.join(save_dir, fname)\n",
    "        gen_new_dir(save_fname)\n",
    "        adc_img = np.squeeze(nib.load(data).get_fdata())\n",
    "        gt_img = nib.load(gt).get_fdata()\n",
    "        adc_img = adc_img.transpose(2,1,0)\n",
    "        gt_img = gt_img.transpose(2,1,0).astype(np.uint8)\n",
    "        adc_save_point = os.path.join(save_fname,'adc')\n",
    "        gt_save_point = os.path.join(save_fname,'mask')\n",
    "        gen_new_dir(adc_save_point)\n",
    "        gen_new_dir(gt_save_point)\n",
    "        for slice_idx in range(len(adc_img)):\n",
    "            np.save(os.path.join(adc_save_point, str(slice_idx).zfill(3)+'.npy'), adc_img[slice_idx])\n",
    "            np.save(os.path.join(gt_save_point, str(slice_idx).zfill(3)+'.npy'), gt_img[slice_idx])\n",
    "        print(f'Saved! {fname}(slices:{len(adc_img)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/ISLES2017_Training'\n",
    "data_path_ls = get_isles_data(train_data_dir, mod='4DPWI') #ADC\n",
    "len(data_path_ls), data_path_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/ISLES2017_Training'\n",
    "save_dir = './data/ISLES2017_Training_2d_DWI'\n",
    "save_isles_dwi_dataset_to_np(save_dir, train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/ISLES2017_Training'\n",
    "save_dir = './data/ISLES2017_Training_2d_ADC'\n",
    "save_isles_adc_dataset_to_np(save_dir, train_data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
