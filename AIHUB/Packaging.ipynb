{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/KOHI/KOHI2021_MedicalImage_Team3/AIHUB\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ./util/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./util/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./util/util.py\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "FILE_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.dcm', '.DCM', '.raw', '.RAW', '.svs', '.SVS']\n",
    "IMG_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "DCM_EXTENSION = ['.dcm', '.DCM']\n",
    "RAW_EXTENSION = ['.raw', '.RAW']\n",
    "NIFTI_EXTENSION = ['.nii']\n",
    "NP_EXTENSION = ['.npy']\n",
    "\n",
    "common_dir = '/home/ncp/workspace/202002n050/050.신경계 질환 관련 임상 및 진료 데이터'\n",
    "\n",
    "\n",
    "def check_extension(filename, extension_ls=FILE_EXTENSION):\n",
    "    return any(filename.endswith(extension) for extension in extension_ls)\n",
    "\n",
    "\n",
    "def load_file_path(folder_path, extension_ls=FILE_EXTENSION, all_sub_folders=False):\n",
    "    \"\"\"find 'IMG_EXTENSION' file paths in folder.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str) -- folder directory\n",
    "        extension_ls (list) -- list of extensions\n",
    "    \n",
    "    Return:\n",
    "        file_paths (list) -- list of 'extension_ls' file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    assert os.path.isdir(folder_path), f'{folder_path} is not a valid directory'\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(folder_path)):\n",
    "        for fname in fnames:\n",
    "            if check_extension(fname, extension_ls):\n",
    "                path = os.path.join(root, fname)\n",
    "                file_paths.append(path)\n",
    "        if not all_sub_folders:\n",
    "            break\n",
    "\n",
    "    return file_paths[:]\n",
    "\n",
    "\n",
    "def gen_new_dir(new_dir):\n",
    "    try: \n",
    "        if not os.path.exists(new_dir): \n",
    "            os.makedirs(new_dir) \n",
    "            #print(f\"New directory!: {new_dir}\")\n",
    "    except OSError: \n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "def find_aihub_img_label_dirs(fname, mod='train'):\n",
    "    if mod == 'train':\n",
    "        img_dir = os.path.join(common_dir, '01.데이터/1.Training/원천데이터', fname, 'init/image')\n",
    "        mask_dir = os.path.join(common_dir, '01.데이터/1.Training/라벨링데이터', fname, 'init/mask')\n",
    "    elif mod == 'val':\n",
    "        img_dir = os.path.join(common_dir, '01.데이터/2.Validation/원천데이터', fname, 'init/image')\n",
    "        mask_dir = os.path.join(common_dir, '01.데이터/2.Validation/라벨링데이터', fname, 'init/mask')\n",
    "    else:\n",
    "        return None\n",
    "    return [img_dir, mask_dir]\n",
    "\n",
    "\n",
    "def pair_img_mask_path(fname, mod='train'):\n",
    "    img_dir, mask_dir = find_aihub_img_label_dirs(fname, mod)\n",
    "    img_path_ls = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
    "    if len(img_path_ls) == 0:\n",
    "        return None\n",
    "    img_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in img_path_ls}\n",
    "    if os.path.isdir(mask_dir):\n",
    "        mask_path_ls = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n",
    "        mask_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in mask_path_ls}\n",
    "    else:\n",
    "        mask_path_dict = {}\n",
    "    paired_list = []\n",
    "    for imgnum, imgpath in img_path_dict.items():\n",
    "        paired_list.append([imgpath, mask_path_dict.get(imgnum)])\n",
    "    return paired_list\n",
    "\n",
    "\n",
    "def find_aihub_img_label_paths(common_dir, mod='train'):\n",
    "    if mod=='train':\n",
    "        data_dir = os.path.join(common_dir, '01.데이터/1.Training/원천데이터')\n",
    "    elif mod=='val':\n",
    "        data_dir = os.path.join(common_dir, '01.데이터/2.Validation/원천데이터')\n",
    "        \n",
    "    _fname = os.listdir(data_dir)\n",
    "    _fname = [p for p in _fname if os.path.isdir(os.path.join(data_dir, p))]\n",
    "    paths_list = []\n",
    "    for fname in _fname:\n",
    "        tmp = pair_img_mask_path(fname, mod)\n",
    "        if tmp:\n",
    "            for p in tmp:\n",
    "                paths_list.append(p)\n",
    "    img_list, mask_list = list(zip(*paths_list))\n",
    "    return img_list, mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./util/visualize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./util/visualize.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize(arr):\n",
    "    tmp = (arr - arr.min())/(arr.max()-arr.min())*255\n",
    "    return tmp.astype(np.uint8)\n",
    "\n",
    "\n",
    "def visualize_grayscale(arr):\n",
    "    tmp = normalize(arr)\n",
    "    return np.stack([tmp, tmp, tmp], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ./data/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/dataloader.py\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "def img_loader(img_path):\n",
    "    return np.expand_dims(np.array(Image.open(img_path)), axis=-1)\n",
    "def mask_loader(mask_path):\n",
    "    return np.expand_dims(np.where(np.array(Image.open(mask_path)),1,0), axis=-1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/dataset_2d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/dataset_2d.py\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "from util.util import *\n",
    "from data.dataloader import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_training_augmentation(params=None):\n",
    "    transform_list = []\n",
    "    \n",
    "    #transform_list.append(A.HorizontalFlip(p=.5))\n",
    "    #transform_list.append(A.VerticalFlip(p=.5))\n",
    "    #transform_list.append(A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=5, shift_limit=0.2, border_mode=0, p=.5))\n",
    "    #transform_list.append(A.ShiftScaleRotate(scale_limit=0.01, rotate_limit=5, shift_limit=0., border_mode=0, p=.5))\n",
    "    \n",
    "    return A.Compose(transform_list)\n",
    "\n",
    "\n",
    "def get_preprocessing(params=None,resize=(256,256),convert=True):\n",
    "    transform_list = []\n",
    "    transform_list.append(A.Resize(*resize))\n",
    "    if convert:\n",
    "        transform_list.append(A.Normalize(mean=(0.5,),  std=(0.5,)))\n",
    "        #transform_list.append(A.Normalize(mean=(0.485, 0.456, 0.406),  std=(0.229, 0.224, 0.225)))\n",
    "        transform_list.append(ToTensorV2(transpose_mask=True))\n",
    "    return A.Compose(transform_list)\n",
    "\n",
    "\n",
    "class AIHUB_LesionSegDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset_dir, \n",
    "                 img_loader=img_loader, \n",
    "                 mask_loader=mask_loader,\n",
    "                 augmentation=None, \n",
    "                 preprocessing=None,\n",
    "                 mode='train'\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.img_loader = img_loader\n",
    "        self.mask_loader = mask_loader\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.mode = mode\n",
    "        self.img_path_ls, self.mask_path_ls = find_aihub_img_label_paths(common_dir, mod=self.mode)\n",
    "        if self.mode != 'train':\n",
    "            self.augmentation = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.img_loader(self.img_path_ls[index])\n",
    "        if self.mask_path_ls[index]:  \n",
    "            mask = self.mask_loader(self.mask_path_ls[index])\n",
    "        else:\n",
    "            mask = np.zeros_like(image)\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
