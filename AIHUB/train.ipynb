{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL.Image as Image\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from util.util import *\n",
    "from util.visualize import *\n",
    "from data.dataset_2d import *\n",
    "\n",
    "common_dir = '/home/ncp/workspace/202002n050/050.신경계 질환 관련 임상 및 진료 데이터'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AIHUB_LesionSegDataset(\n",
    "    dataset_dir=comon_dir, \n",
    "    augmentation=None, \n",
    "    preprocessing=get_preprocessing(resize=(256,256)),\n",
    "    mode='train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = AIHUB_LesionSegDataset(\n",
    "    dataset_dir=comon_dir, \n",
    "    augmentation=None, \n",
    "    preprocessing=get_preprocessing(resize=(256,256)),\n",
    "    mode='val'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset = AIHUB_LesionSegDataset(\n",
    "    dataset_dir=comon_dir, \n",
    "    augmentation=None, \n",
    "    preprocessing=get_preprocessing(resize=(256,256), convert=False),\n",
    "    mode='val'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check augmentation \n",
    "for i in range(10,12):\n",
    "    image, mask = aug_dataset[i] \n",
    "    visualize(image=visualize_grayscale(np.squeeze(image)), mask=visualize_grayscale(np.squeeze(mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./ADC_ckpt/2d_ckpt/UNet_resnet152\"\n",
    "gen_new_dir(save_path)\n",
    "###############################\n",
    "trial = 1\n",
    "n_epoches = 10000\n",
    "LR = 0.0001\n",
    "LR_DECREASE = 1e-5\n",
    "lr_decrease_epoch = 70\n",
    "BATCH_SIZE = 64\n",
    "patience= 15\n",
    "###############################\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, \n",
    "                                         shuffle=False)\n",
    "\n",
    "ENCODER = 'resnet152'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=None, \n",
    "    in_channels=1,\n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=LR),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "with open(os.path.join(save_path, f'results{str(trial).zfill(2)}.csv'), 'w') as f:\n",
    "    f.write('epoch,train_loss,train_score,valid_loss,valid_score\\n')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "for epoch in range(0, n_epoches):\n",
    "    \n",
    "    print(f'\\nEpoch: {epoch}')\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(val_loader)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'results{str(trial).zfill(2)}.csv'), 'a') as f:\n",
    "            f.write('%03d,%0.6f,%0.6f,%0.6f,%0.6f\\n' % (\n",
    "                (epoch + 1),\n",
    "                train_logs['dice_loss'],\n",
    "                train_logs['iou_score'],\n",
    "                valid_logs['dice_loss'],\n",
    "                valid_logs['iou_score'],\n",
    "            ))\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, os.path.join(save_path, f'best_model{str(trial).zfill(2)}.pth'))\n",
    "        print('New Record!')\n",
    "        \n",
    "    torch.save(model, os.path.join(save_path, f'final_model{str(trial).zfill(2)}.pth'))\n",
    "    \n",
    "    early_stopping(valid_logs['dice_loss'], model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "    if epoch == lr_decrease_epoch:\n",
    "        optimizer.param_groups[0]['lr'] = LR_DECREASE\n",
    "        print(f'Decrease decoder learning rate to {LR_DECREASE}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = AIHUB_LesionSegDataset(\n",
    "    dataset_dir=comon_dir, \n",
    "    augmentation=None, \n",
    "    preprocessing=get_preprocessing(resize=(256,256)),\n",
    "    mode='val'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_val_dataset = AIHUB_LesionSegDataset(\n",
    "    dataset_dir=comon_dir, \n",
    "    augmentation=None, \n",
    "    preprocessing=get_preprocessing(resize=(256,256), convert=False),\n",
    "    mode='val'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "save_path = \"./ADC_ckpt/2d_ckpt/UNet_resnet152\"\n",
    "best_model = torch.load(os.path.join(save_path, 'best_model01.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = pd.read_csv(os.path.join(save_path,'results01.csv'))\n",
    "fig,ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(np.array(train_history['train_loss']), 'b')\n",
    "ax[0].plot(np.array(train_history['valid_loss']), 'r')\n",
    "\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(np.array(train_history['train_score']), 'b')\n",
    "ax[1].plot(np.array(train_history['valid_score']), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_masks = []\n",
    "\n",
    "for data in val_loader:\n",
    "    images, labels = data\n",
    "    images = images.to(DEVICE)\n",
    "    masks = labels.to(DEVICE)\n",
    "    pr_mask = best_model.predict(images)\n",
    "    predict_masks.append(pr_mask.cpu().numpy().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_masks = np.squeeze(np.vstack(predict_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#red:FalsePositive / green:TruePositive / blue:FalseNegative\n",
    "for i in range(0,63):\n",
    "    image, mask = vis_val_dataset[i] \n",
    "    predict= predict_masks[i]\n",
    "    image_rgb = visualize_grayscale(np.squeeze(image))\n",
    "    predict= predict.astype(np.uint8)\n",
    "    predict= predict[:,:,np.newaxis]\n",
    "    intersect_mask = mask*predict\n",
    "    only_mask = np.where((mask-intersect_mask)==1, 1, 0)\n",
    "    only_pred = np.where((predict-intersect_mask)==1, 1, 0)\n",
    "    tp_np_mask = np.concatenate([only_pred,intersect_mask,only_mask], axis=-1)*255\n",
    "    vis = image_rgb/2 + tp_np_mask/2\n",
    "    vis = vis.astype(np.uint8)\n",
    "    visualize(image=image_rgb, result=tp_np_mask, visualize= vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code reference: https://gist.github.com/gergf/acd8e3fd23347cb9e6dc572f00c63d79\n",
    "def dice(true_mask, pred_mask, non_seg_score=0.0):\n",
    "    \"\"\"\n",
    "        Computes the Dice coefficient.\n",
    "        Args:\n",
    "            true_mask : Array of arbitrary shape.\n",
    "            pred_mask : Array with the same shape than true_mask.  \n",
    "        \n",
    "        Returns:\n",
    "            A scalar representing the Dice coefficient between the two segmentations. \n",
    "        \n",
    "    \"\"\"\n",
    "    assert true_mask.shape == pred_mask.shape\n",
    "\n",
    "    true_mask = np.asarray(true_mask).astype(np.bool_)\n",
    "    pred_mask = np.asarray(pred_mask).astype(np.bool_)\n",
    "\n",
    "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
    "    im_sum = true_mask.sum() + pred_mask.sum()\n",
    "    if im_sum == 0:\n",
    "        return non_seg_score\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(true_mask, pred_mask)\n",
    "    return 2. * intersection.sum() / im_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_avg = 0\n",
    "cnt = 0\n",
    "for i in range(len(vis_val_dataset)):\n",
    "    image, mask = vis_val_dataset[i] \n",
    "    if (predict_masks[i].max() != 0.) & (mask.max() != 0.):\n",
    "        dice_avg += dice(np.squeeze(mask.astype(np.uint8)), predict_masks[i].astype(np.uint8))\n",
    "        cnt += 1\n",
    "    else:\n",
    "        pass\n",
    "dice_avg /= cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
