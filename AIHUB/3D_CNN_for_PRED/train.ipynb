{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import glob\n",
    "\n",
    "FILE_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.dcm', '.DCM', '.raw', '.RAW', '.svs', '.SVS']\n",
    "IMG_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "DCM_EXTENSION = ['.dcm', '.DCM']\n",
    "RAW_EXTENSION = ['.raw', '.RAW']\n",
    "NIFTI_EXTENSION = ['.nii']\n",
    "NP_EXTENSION = ['.npy']\n",
    "\n",
    "\n",
    "def check_extension(filename, extension_ls=FILE_EXTENSION):\n",
    "    return any(filename.endswith(extension) for extension in extension_ls)\n",
    "\n",
    "\n",
    "def load_file_path(folder_path, extension_ls=FILE_EXTENSION, all_sub_folders=False):\n",
    "    \"\"\"find 'IMG_EXTENSION' file paths in folder.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str) -- folder directory\n",
    "        extension_ls (list) -- list of extensions\n",
    "    \n",
    "    Return:\n",
    "        file_paths (list) -- list of 'extension_ls' file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    assert os.path.isdir(folder_path), f'{folder_path} is not a valid directory'\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(folder_path)):\n",
    "        for fname in fnames:\n",
    "            if check_extension(fname, extension_ls):\n",
    "                path = os.path.join(root, fname)\n",
    "                file_paths.append(path)\n",
    "        if not all_sub_folders:\n",
    "            break\n",
    "\n",
    "    return file_paths[:]\n",
    "\n",
    "\n",
    "def gen_new_dir(new_dir):\n",
    "    try: \n",
    "        if not os.path.exists(new_dir): \n",
    "            os.makedirs(new_dir) \n",
    "            #print(f\"New directory!: {new_dir}\")\n",
    "    except OSError: \n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "\n",
    "def get_data_fname_label_in_split(data_df, mode='train'):\n",
    "    return data_df[data_df['split']==mode][['name', 'good_outcome_3m']].values\n",
    "\n",
    "\n",
    "def get_dataset(data_df, data_dir, mode='train'):\n",
    "    data_fname_label_arr = get_data_fname_label_in_split(data_df, mode=mode)\n",
    "    np_path_ls = sorted(load_file_path(data_dir, NP_EXTENSION))\n",
    "    np_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in np_path_ls}\n",
    "    return [[np_path_dict.get(fname), label] for fname, label in data_label_arr if np_path_dict.get(fname)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    tmp = (arr - arr.min())/(arr.max()-arr.min())\n",
    "    return tmp.astype(np.float32)\n",
    "\n",
    "def img_loader(imgpath):\n",
    "    img = np.load(imgpath).transpose(2,1,0)\n",
    "    return normalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_training_augmentation(params=None):\n",
    "    transform_list = []\n",
    "    \n",
    "    #transform_list.append(A.HorizontalFlip(p=.5))\n",
    "    #transform_list.append(A.VerticalFlip(p=.5))\n",
    "    #transform_list.append(A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=5, shift_limit=0.2, border_mode=0, p=.5))\n",
    "    #transform_list.append(A.ShiftScaleRotate(scale_limit=0.01, rotate_limit=5, shift_limit=0., border_mode=0, p=.5))\n",
    "    \n",
    "    return A.Compose(transform_list)\n",
    "\n",
    "\n",
    "def get_preprocessing(params=None,convert=True):\n",
    "    transform_list = []\n",
    "    if convert:\n",
    "        #transform_list.append(A.Normalize(mean=(0.5,),  std=(0.5,)))\n",
    "        #transform_list.append(A.Normalize(mean=(0.485, 0.456, 0.406),  std=(0.229, 0.224, 0.225)))\n",
    "        transform_list.append(ToTensorV2(transpose_mask=True))\n",
    "    return A.Compose(transform_list)\n",
    "\n",
    "\n",
    "class AIHUB_GoodOutcomPredDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset_dir, \n",
    "                 dataset_df,\n",
    "                 img_loader=img_loader, \n",
    "                 augmentation=None, \n",
    "                 preprocessing=None,\n",
    "                 mode='train'\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.dataset_df = pd.read_csv(dataset_df)\n",
    "        self.img_loader = img_loader\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.mode = mode\n",
    "        self.dataset = get_dataset(self.dataset_df, self.dataset_dir, self.mode)\n",
    "        if self.mode != 'train':\n",
    "            self.augmentation = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.dataset[index]\n",
    "        image = img_loader(image_path)\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image = sample['image']\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            image = torch.unsqueeze(image, 0).permute(0, 1, 2, 3)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "    dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "    dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "    augmentation=None,\n",
    "    preprocessing=get_preprocessing(),\n",
    "    mode='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "    dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "    dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "    augmentation=None,\n",
    "    preprocessing=get_preprocessing(),\n",
    "    mode='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "    dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "    dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "    augmentation=None,\n",
    "    preprocessing=get_preprocessing(),\n",
    "    mode='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=100):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        if torch.cuda.is_available():\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        #loss = criterion(output, target)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.data.cpu().topk(1, dim=1)\n",
    "        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print stats\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
    "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
    "                'Error %.4f (%.4f)' % (error.val, error.avg),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    # Return summary statistics\n",
    "    return batch_time.avg, losses.avg, error.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, loader, print_freq=10, is_test=True):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            # Create vaiables\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            #loss = criterion(output, target)\n",
    "            loss = torch.nn.functional.cross_entropy(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = target.size(0)\n",
    "            _, pred = output.data.cpu().topk(1, dim=1)\n",
    "            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            losses.update(loss.item(), batch_size)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # print stats\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
    "                    'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
    "                    'Error %.4f (%.4f)' % (error.val, error.avg),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    # Return summary statistics\n",
    "    return batch_time.avg, losses.avg, error.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, valid_set, test_set, save, n_epochs=300,\n",
    "          batch_size=64, lr=0.0001, patience=10, save_epoch=10, seed=None):\n",
    "    cnt=0\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=batch_size, drop_last=True, shuffle=True,\n",
    "                                               pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False,\n",
    "                                                   pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    # Model on cuda\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs],\n",
    "                                                     gamma=0.1)\n",
    "\n",
    "    # Start log\n",
    "    with open(os.path.join(save, 'results.csv'), 'w') as f:\n",
    "        f.write('epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n')\n",
    "\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss, train_error = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        scheduler.step()\n",
    "        _, valid_loss, valid_error = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=valid_loader if valid_loader else test_loader,\n",
    "            is_test=(not valid_loader)\n",
    "        )\n",
    "\n",
    "        # Determine if model is the best\n",
    "        if valid_loader:\n",
    "            if valid_error < best_error:\n",
    "                best_error = valid_error\n",
    "                print('New best error: %.4f' % best_error)\n",
    "                torch.save(model.state_dict(), os.path.join(save, 'model_best.dat'))\n",
    "        else:\n",
    "            if (cnt%save_epoch==0):\n",
    "                #torch.save(model.state_dict(), os.path.join(save, 'model_epoch'+str(cnt).zfill(3)+'.dat'))\n",
    "                pass\n",
    "        # Log results\n",
    "        with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
    "            f.write('%03d,%0.6f,%0.6f,%0.5f,%0.5f,\\n' % (\n",
    "                (epoch + 1),\n",
    "                train_loss,\n",
    "                train_error,\n",
    "                valid_loss,\n",
    "                valid_error,\n",
    "            ))\n",
    "        cnt+=1\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    torch.save(model.state_dict(), os.path.join(save, 'model_final.dat'))\n",
    "\n",
    "    # Final test of model on test set\n",
    "    model.load_state_dict(torch.load(os.path.join(save, 'model_final.dat')))\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    test_results = test_epoch(\n",
    "        model=model,\n",
    "        loader=test_loader,\n",
    "        is_test=True\n",
    "    )\n",
    "    _, _, test_error = test_results\n",
    "    with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
    "        f.write(',,,,,%0.5f\\n' % (test_error))\n",
    "    print('Final test error: %.4f' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(save, model,\n",
    "         n_epochs=300, \n",
    "         batch_size=64, \n",
    "         lr=0.0001, \n",
    "         patience=10, \n",
    "         seed=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Datasets\n",
    "    train_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "        augmentation=None,\n",
    "        preprocessing=get_preprocessing(),\n",
    "        mode='train')\n",
    "    val_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "        augmentation=None,\n",
    "        preprocessing=get_preprocessing(),\n",
    "        mode='val')\n",
    "    test_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "        augmentation=None,\n",
    "        preprocessing=get_preprocessing(),\n",
    "        mode='test')\n",
    "\n",
    "    # Models\n",
    "    #print(model)\n",
    "    \n",
    "    # Print number of parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Total parameters: \", num_params)\n",
    "\n",
    "    # Make save directory\n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    if not os.path.isdir(save):\n",
    "        raise Exception('%s is not a dir' % save)\n",
    "\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_dataset, valid_set=val_dataset, test_set=test_dataset, save=save,\n",
    "          n_epochs=n_epochs, batch_size=batch_size, lr=lr, patience=patience, seed=seed)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import resnet, wide_resnet, resnext, densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './3DResNet101'\n",
    "gen_new_dir(save_path)\n",
    "N_EPOCHS = 10000\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00001\n",
    "PATIENCE = 10\n",
    "\n",
    "model = resnet.resnet101(num_classes=2, \n",
    "                         shortcut_type='A', \n",
    "                         spatial_size=256, \n",
    "                         sample_duration=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo(save=save_path, \n",
    "     model=model,\n",
    "     n_epochs=N_EPOCHS, \n",
    "     batch_size=BATCH_SIZE, \n",
    "     lr=LR, \n",
    "     patience=PATIENCE, \n",
    "     seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(testloader, model, threshold=0.5):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    output_arr = np.ones((1, 2))\n",
    "    label_arr = np.array([])\n",
    "    pred_arr = np.array([])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) # argmax\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            output_arr = np.concatenate((output_arr, outputs.softmax(1).cpu().numpy()), axis=0)\n",
    "            label_arr = np.concatenate((label_arr, labels.cpu().numpy()), axis=0)\n",
    "            pred_arr = np.concatenate((pred_arr, predicted.cpu().numpy()), axis=0)\n",
    "\n",
    "    output_arr = np.delete(output_arr, 0, axis=0)\n",
    "    acc = correct/total\n",
    "    print('Accuracy on the test images: ', (100*correct/total))\n",
    "    return acc, output_arr, label_arr, pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './3DResNet101'\n",
    "\n",
    "test_model = resnet.resnet101(num_classes=2, \n",
    "                         shortcut_type='A', \n",
    "                         spatial_size=256, \n",
    "                         sample_duration=36)\n",
    "test_model.load_state_dict(torch.load(os.path.join(save_path, 'model_best.dat')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AIHUB_GoodOutcomPredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/data_np_resampled',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/3D_CNN_for_PRED/aihub_df.csv',\n",
    "        augmentation=None,\n",
    "        preprocessing=get_preprocessing(),\n",
    "        mode='test')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(intest_dataset, \n",
    "                                          batch_size=16, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=(torch.cuda.is_available()), \n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, output_arr, label_arr, pred_arr = test_acc(intest_loader, test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = output_arr.copy()\n",
    "threshold = 0.962\n",
    "tmp[:,0] -= (threshold-0.5) / 2\n",
    "tmp[:,1] += (threshold-0.5) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = []\n",
    "for t in tmp:\n",
    "    pred_.append(np.argmax(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(label_arr, pred_, target_names=['survived', 'not_survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bad_outcome', 'good_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(label_arr, pred_)\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in class_names], columns = [i for i in class_names])\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "#plt.ylabel('True label')\n",
    "#plt.xlabel('Pred label')\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(label_arr == i, output_arr[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "roc_auc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr[0], tpr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
